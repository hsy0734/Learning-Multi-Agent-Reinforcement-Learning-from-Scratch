该方法只是适用于回合制的任务

布模型：表示概率分布的模型

样本模型：只需要采样的模型

```plain
import numpy as np

def sample(dices=2):
    for _ in range(dices):
        x+=np.random.choice([1,2,3,4,5])
    return x
```

```plain
trial=1000

samples=[]
for _ in range(trial):
    s=sample()
    samples.append(s)

V=sum(samples)/len(samples)#求平均值
print(V)
```

当几次任务遍历了所有状态，来计算收益的期望值，利用倒退方式来求各个状态的收益，因为可以重复利用已经计算好的值，以至于简化计算

```plain
from common.gridworld import GridWorld

env=GridWorld()
action =0
next_state,reward,done=env.step(action)#done是回合结束的标志位
```

```plain
from collections import defaultdict


class RandomAgent:
    def __init__(self):
        self.gamma=0.9
        self.action_size=4

        random_actions={0:0.25,1:0.25,2:0.25,3:0.25}
        self.pi=defaultdict(lambda:random_actions)
        self.V=defaultdict(lambda:0)
        self.cnts=defaultdict(lambda:0)
        self.memory=[]

    def get_action(self,state):
        action_probs=self.pi[state]
        actions=list(action_probs.keys())
        probs=list(action_probs.values())
        return np.random.choice(actions,p=probs)
```

蒙特卡特策略控制就是像老虎机一样有两个准则，第一是利用和探索同时兼顾，第二是在非稳态环境中，最新的奖励比早的奖励权重更大





同策略型基于自己的经验改进自己策略的做法

异策略型基于其他地方获得的经验改进自己的策略的做法

目标策略：作为评估和改进对象的策略

行为策略：产生状态，行动和奖励的样本数据



这里主题是异策略型，根据行为策略获得的经验来评估和改进目标策略，让行为策略进行探索，目标策略进行利用，而在从行为策略获得的样本数据来求目标策略的期望值时，需要利用计算技巧为重要性采样



重要性采样：利用其他概率分布采样的数据来计算某个概率分布的期望值

减少方差的方法：找概率分布接近的模型



正在研究的课题：基于行为策略的概率分布采样的数据可以用来计算目标策略的期望值——附录 A

