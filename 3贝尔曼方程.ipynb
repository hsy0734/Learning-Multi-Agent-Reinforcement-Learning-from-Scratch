{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2aca01",
   "metadata": {},
   "source": [
    "联合概率：p（x，y）=p（x）p（y|x），xy同时发生的概率\n",
    "贝尔曼方程：把无限的计算期望转为有限的联立方程（非常屌，可惜不能用于实际因为计算量太大了）\n",
    "行动价值函数：就是在状态价值函数的基础上把行动a变成变量，不由π决定，所以价值函数=q函数*策略派对a的累加\n",
    "最优策略：是使所有状态下的状态价值函数最大化的策略\n",
    "状态价值函数的贝尔曼最优方程：使用max算子表示贝尔曼方程就是贝尔曼最优方程m\n",
    "Q函数的贝尔曼最优方程：也是使用max算子来天魂策略π\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00beb7a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c8ca1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
