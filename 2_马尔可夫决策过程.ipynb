{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdeaba7f",
   "metadata": {},
   "source": [
    "马尔可夫决策过程\n",
    "状态迁移函数：s'=f（s，a）是一个给定状态s和行动a输出下一个状态s‘的函数\n",
    "状态迁移概率：p（s'|s，a）是agent在a行动之后从s迁移到s’状态的概率\n",
    "马尔可夫性：状态的迁移不需要过去的信息，只需要此前的状态和执行了什么行动\n",
    "奖励函数：r（s，a，s‘）是agent处于状态s，执行行动a，下一个状态是s’时\n",
    "确定性策略：a=μ（s）当agent处于某个状态时总是采取同样的行动\n",
    "随机性策略：π（a|s）当agent处于某个状态时可以采取不同的动作\n",
    "收益：Gt=Rt+γRt+1+γRt+2+......设置折现率是为了防止连续性的任务的收益变的无穷大\n",
    "状态价值函数：收益的期望值Vπ（s）=Eπ【Gt|St=s】，在t时刻状态为s，agent的策略为π，收益的期望值为Vπ（s）\n",
    "最优策略：在任何状态下该策略的收益都是最好的\n",
    "最优价值函数：V*最优策略的状态价值函数"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
