defaultdict 的用法

目的是解放烦琐的初始化工作，当查找不属于这里的键的时候会返回一个 0

```plain
from collection import defaultdict
from common.gridworld improt GridWorld

env=GridWorld()
V=defaultdict(lambda:0)

state=(1,2)
print(V[state])

```

```plain
pi=defaultdict(lambda:{0:0.25,1:0.25,2:0.25,3:0.25})#随机性策略，就在输入任意一个状态都会返回该状态下的行动概率分布

state=(0,1)
print(pi[state])
```

迭代策略评估的实现

```plain
def eval_onestep(pi,V,env,gamma=0.9):
    for state in env.state():
        if state==env.goal_state:
            V[state]=0
            continue

        action_probs=pi[state]
        new_V=0

        for action,action_prob in action_probs.items():
            next_state=env.next_state(state,action)
            r=env.reward(state,action,next_state)
            new_V+=action_prob*(r+gamma*V[next_state])
            V[state]=new_V
return V
```

既然实现了一次训练的代码，那就可以开始循环训练直到更新量到达阈值

```plain
def policy_eval(pi,V,env,gamma,threshould=0.001):#输入策略pi就是在这个状态下有哪些待选项，V是价值函数就是这个状态下的期望价值，env是模拟环境，gamma是折现率表示未来的价值会打折扣，threshould是更新率控制着代码什么时候停下来
    while True:#一直循环直到符合条件退出
        old_V=V.copy()#更新前的价值函数
        V=eval_onestep(pi,V,env,gamma)#计算这个状态下新的价值函数


        #求更新量的最大值
        delta=0
        for state in V.keys():
            t=abs(V[state-old_V[state]])
            if delta<t:
                delta=t

        if delta<threshould:
            break
    return V
```

```plain
env=GridWorld()
gamma=0.9

pi=defaultdict(lambda:{0:0.25,1:0.25,2:0.25,3:0.25})
V=defaultdict(lambda:0)

V=policy_eval(pi,V,env,gamma)
env.render_v(V,pi)
```

这个时候各个状态的价值函数都有了，但是我们需要去寻找最优策略，目前我们有一种方法就是求解贝尔曼最优方程的联立解，但是这个方法的计算量很大，所以我们需要先对策略进行评估然后再对其进行改进，所以我们接下来学习改进策略的方法



接下来我们了解几个概念

最优策略

最优策略的状态价值函数

最优策略的行动价值函数

其中最优策略就是行动价值函数中最大的那一个，现在有一个有意思的问题，就是如果要求最优策略，那就得直到最优策略的状态价值函数，但是要求最优状态价值函数就得知道最优策略，成死循环了

不急，首先我们要知道策略的贪婪化意味着策略总是被改进，如果策略没有被改进，那它就是最优策略

所以需要重复评估和改进

例如从 π0 策略开始然后用迭代评估策略来评估他的价值，然后用价值 V 来求最优策略 π1 依次类推

这种方法叫做**策略迭代法**

```plain
#他会接受字典为参数并返回具有最大字典值的键
action_value={0:1,1:-1}
max_action=argmax(action_value)
print(max_action)#答案为0
```

```plain
def argmax(d):
    max_value=max(d.value())
    max_key=0
    for key,value in d.items():
        if value==max_value:
            max_key=key
    return max_key
```

```plain
def greedy_policy(V,env,gamma):
    pi={}

    for state in env.states():#遍历所有状态
        action_value={}

        for action in env.actions():#把这个状态的所有行动的价值都计算一遍
            next_state=env.next_state(state,action)
            r=env.reward(state,action,next_state)
            value=r+gamma*V[next_state]
            action_values[action]=value
        max_action=argmax(action_values)#利用argmax求解价值最大的行动，行动是键，价值是值，返回出这个状态最大价值的键
        action_probs={0:0,1:0,2:0,3:0}
        action_probs[max_action]=1.0
        pi[state]=action_probs#在python中字典可以作为另一个字典的值
    return pi
```

```plain
from email.policy import default
def policy_iter(env,gamma,threshould=0.01,is_render=False):
    pi=defaultdict(lambda:{0:0.25,1:0.25,2:0.25,3:0.25})
#这行代码创建了一个名为 pi 的默认字典，用来表示策略（Policy），核心逻辑：
# pi 的键：代表环境中的状态（比如格子世界的坐标 (0,0)、(1,2)）；
# pi 的值：是一个字典，代表该状态下每个动作的选择概率；
# 当访问一个不存在的状态时（比如新状态 s_new），defaultdict 会自动调用 lambda 函数，为这个状态生成 “4 个动作各 25% 概率” 的默认策略。
    V=defaultdict(lambda:0)
    while True:
        V=policy_eval(pi,V,env,threshould)#评估
        new_pi=greedy_policy(V,env,gamma)#改进

        if is_render:
            env.render_v(V,pi)
        
        if new_pi=pi:
            break
    return pi
    
```

通用策略迭代：在到达两条线之前蜿蜒前行

在评估阶段只需要越来越接近即可，在改进阶段只需朝着贪婪化方向前进即可



价值迭代法：每当更新一个状态之后就立刻改进策略，然后将式子结合之后只剩下一个价值函数式子

