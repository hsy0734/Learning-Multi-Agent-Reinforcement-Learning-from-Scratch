同策略型 sarsa 方法：用什么策略选动作就用什么策略

这个方法有一个核心公式

<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/png/38602209/1770964068939-120d220d-935f-4dc5-b9cc-a2bc77e376b8.png)

动作价值函数=待更新的动作价值函数+学习率*[ 下一步的奖励+未来的奖励（因为是未来的奖励所以需要打折）-待更新的动作价值函数值 ]



<!-- 这是一张图片，ocr 内容为： -->
![](https://cdn.nlark.com/yuque/0/2026/png/38602209/1770964308200-7fcc9c62-5021-4e06-8006-d68d9b012670.png)

这个叫做 TD error(时序差分误差)



异策略型 sarsa 方法（Q 学习）：

在同策略和异策略的比较中，在选择动作时都一样，但是在更新 q 值时，前者用实际选的（有可能差，因为有一定概率是去探索而不是贪婪化的利用），后者用最理想的也就是最贪婪的



